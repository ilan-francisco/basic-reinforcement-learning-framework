{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning Final Project\n",
    "\n",
    "Student: **Ilan Francisco da Silva Theodoro** (ra257199)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb3a90bfe2a759b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this project, we will implement and compare the performance of the following algorithms:\n",
    "- Monte Carlo Control\n",
    "- Sarsa($\\lambda$)\n",
    "- Q-Learning\n",
    "- Deep Q-Learning\n",
    "\n",
    "But first, we will implement a few helper functions and classes. Since there are a lot of redundancy between the algorithms, we will implement a base class for the control algorithms, and then we will implement the specific algorithms as subclasses of this base class. The same happens for the Q-functions, where we will implement a base class for the Q-functions, and then we will implement the specific Q-functions as subclasses of this base class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "719fde717688a979"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing dependencies\n",
    "\n",
    "Firstly, we need to install the required packages:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0d3e75849333d24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install gymnasium==0.29.1\n",
    "!pip install scikit-learn==1.3.2\n",
    "!pip install tqdm==4.66.1\n",
    "!pip install numba==0.58.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a56e814a59b2be2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Environment state normalization\n",
    "\n",
    "To start, we will implement a class to normalize the environment's state space. This is useful for the linear function approximation, as it will allow us to use the same weights for different environments.\n",
    "\n",
    "It is a normalizer that scales the state space to the interval $[-1, 1]$. It is based on the following formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{x} &= \\frac{2\\sigma(x) + \\sigma(x_{max}) + \\sigma(x_{min})}{\\sigma(x_{max}) - \\sigma(x_{min})} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $x_{max}$ and $x_{min}$ are the maximum and minimum values of the state space, respectively. $\\sigma$ is the sigmoid function. This formula is applied to each dimension of the state space. It enforces that the state space is scaled to the interval $[-1, 1]$.\n",
    "\n",
    "It is slightly different from how is calculated in the code. This happens because the above formula is a simplification that I got while I was writing this report. I got messy when I was writing the code kkkk."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd6c7bbe8fe537d0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"This module contains the environment tools used in the project.\"\"\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "\n",
    "class EnvironmentNormalizer(gym.ObservationWrapper):\n",
    "    \"\"\"Normalized environment.\n",
    "\n",
    "    This class is used to normalize the state of an environment according to a\n",
    "    sigmoid function. It scales any observation value-range to the interval\n",
    "    [-1, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        \"\"\"Creates a new normalized environment.\n",
    "\n",
    "        :param env: the environment to be normalized.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=self.env.observation_space.shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        if isinstance(self.env.observation_space, gym.spaces.Box):\n",
    "            self.scale_right = sigmoid(self.env.observation_space.high) - 0.5\n",
    "            self.scale_left = sigmoid(self.env.observation_space.low) - 0.5\n",
    "            self.mean = (self.scale_right + self.scale_left) / 2\n",
    "\n",
    "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalized observation.\n",
    "\n",
    "        :param observation: the observation to be normalized.\n",
    "        :returns: the normalized observation.\n",
    "        \"\"\"\n",
    "        return self._normalize(observation)\n",
    "\n",
    "    def _normalize(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize the state according to a sigmoid function.\"\"\"\n",
    "        obs = sigmoid(obs) - 0.5\n",
    "        obs = (obs - self.mean) / (self.scale_right - self.mean)\n",
    "        return obs\n",
    "\n",
    "    @staticmethod\n",
    "    def from_gym(name_id: str) -> gym.Env:\n",
    "        \"\"\"Create a new normalized environment from gym.\n",
    "\n",
    "        :param name_id: name of the environment in gym.\n",
    "        :returns: a new normalized environment.\n",
    "        \"\"\"\n",
    "        return EnvironmentNormalizer(gym.make(name_id))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T14:40:20.949880867Z",
     "start_time": "2023-12-19T14:40:20.623890635Z"
    }
   },
   "id": "b39b14283ed1b416"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-functions\n",
    "\n",
    "Now, we will implement the Q-functions. We will implement two Q-functions: one tabular and one linear. The tabular Q-function is the simplest one, and it is used as a baseline for the other Q-functions. The linear Q-function is a linear approximation of the tabular Q-function. It is used to reduce the number of parameters of the Q-function, which is useful for environments with large state spaces.\n",
    "\n",
    "The most important methods of the Q-function are:\n",
    "- `__call__`: returns the Q-value of a state-action pair.\n",
    "- `update`: updates the Q-value of a state-action pair according to the\n",
    "  update rule of the Q-function.\n",
    "- `q_max`: returns the maximal Q-value for a given state, and the action\n",
    "  associated with it. It is used to choose the action according to the greedy\n",
    "    policy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adb1cad2b0c23689"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tabular Q-function\n",
    "\n",
    "The tabular Q-function is implemented as a numpy array since we know how many states the model has and the number of actions, because the environment is normalized and can be easily discretized.\n",
    "\n",
    "In that sense, each state is discretized and directly mapped to the Q-value array. Each state dimension sum up to a new dimension in the Q-value array, in the sense that if $N$ is the number of possible values for a state in a dimension, then the Q-value array will have a shape of $(N, N, ..., N, A)$, where $A$ is the number of actions available in the environment.  \n",
    "\n",
    "Each state dimension is discretized according to the following formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{x} &= \\text{round}((x + 1) * \\alpha) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the discrete scale. It is a hyperparameter that defines the number of possible values for a state in a dimension. It is used to discretize the state space to a finite number of values. It is a trade-off between the number of states and the accuracy of the Q-function. The higher the discrete scale, the higher the number of states, and the higher the accuracy of the Q-function.\n",
    "\n",
    "The state $x$ is summed with $1$ to ensure that the state is always positive. This is necessary because the state can be negative down to $-1$, and the index of the Q-value array must be positive. The state is multiplied by $\\alpha$ to scale the state to the interval $[0, 2*\\alpha]$. Then, the state is rounded to the nearest integer to get the index of the Q-value array. Hence, $N = 2*\\alpha + 1$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88efb087fc9fc169"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from typing import Any\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "\n",
    "class QTabular:\n",
    "    \"\"\"Discretized tabular Q state-action-value function.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_actions: int, n_feat: int, discrete_scale: float = 40.0\n",
    "    ) -> None:\n",
    "        \"\"\"Define a tabular Q function.\n",
    "\n",
    "        :param n_actions: number of actions available in the environment.\n",
    "        :param n_feat: number of features in the state.\n",
    "        :param discrete_scale: scale in which the state will be discretized. The\n",
    "         state will be multiplied by this number and then rounded to the nearest\n",
    "         integer.\n",
    "        \"\"\"\n",
    "        self._q = np.zeros(\n",
    "            (*[discrete_scale * 2 + 1] * n_feat, n_actions), dtype=np.float32\n",
    "        )\n",
    "        self._n = np.zeros(\n",
    "            (*[discrete_scale * 2 + 1] * n_feat, n_actions), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.n_feat = n_feat\n",
    "        self.count_non_zero = 0\n",
    "        self.discrete_scale = discrete_scale\n",
    "\n",
    "    def __call__(self, state: np.ndarray, action: int) -> float:\n",
    "        \"\"\"Call function to get the Q value of a state-action pair.\"\"\"\n",
    "        state = self._preprocess_state(state)\n",
    "        idx = self._index(state, action)\n",
    "        return self._q[idx].item()\n",
    "\n",
    "    @property\n",
    "    def states_explored(self) -> int:\n",
    "        \"\"\"Number of states explored.\"\"\"\n",
    "        return self.count_non_zero\n",
    "\n",
    "    def q_max(self, state: np.ndarray) -> tuple[int, float]:\n",
    "        \"\"\"Maximal Q value for a given state.\n",
    "\n",
    "        Get the action and Q value for the maximal Q value for a given\n",
    "        state.\n",
    "\n",
    "        :param state: state to be evaluated.\n",
    "        :return: action and Q value for the maximal Q value for a given state.\n",
    "        \"\"\"\n",
    "        state = self._preprocess_state(state)\n",
    "        idx_state = self._index(state)\n",
    "\n",
    "        maximal_value = -np.inf\n",
    "        maximal_set = []\n",
    "        for action in range(self.n_actions):\n",
    "            if maximal_value < self._q[idx_state][action]:\n",
    "                maximal_value = self._q[idx_state][action]\n",
    "                maximal_set = [action]\n",
    "            elif maximal_value == self._q[idx_state][action]:\n",
    "                maximal_set.append(action)\n",
    "\n",
    "        action = np.random.choice(maximal_set)\n",
    "\n",
    "        return action, maximal_value\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        expected: float,\n",
    "        _: float,\n",
    "        α: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Update the Q value for a given state-action pair.\n",
    "\n",
    "        It is a classical update rule for a tabular Q function. The update rule\n",
    "        follows the principle of minimizing the error between the expected and\n",
    "        the predicted value.\n",
    "\n",
    "        :param state: state associated.\n",
    "        :param action: action associated.\n",
    "        :param expected: expected value for the evaluated policy.\n",
    "        :param _: ignored.\n",
    "        :param α: learning rate.\n",
    "        \"\"\"\n",
    "        predicted = self(state, action)\n",
    "        state = self._preprocess_state(state)\n",
    "        idx = self._index(state, action)\n",
    "        if self._n[idx] == 0:\n",
    "            self.count_non_zero += 1\n",
    "        self._n[idx] += α\n",
    "        self._q[idx] += α * (expected - predicted)\n",
    "\n",
    "    def n(self, state: np.ndarray, action: Optional[int] = None) -> float:\n",
    "        \"\"\"Number of times a state or a state-action pair was visited.\n",
    "\n",
    "        :param state: state to be evaluated.\n",
    "        :param action: action to be evaluated. If None, returns the number of\n",
    "         times the state was visited.\n",
    "        :return: number of times a state or a state-action pair was visited.\n",
    "        \"\"\"\n",
    "        state = self._preprocess_state(state)\n",
    "        idx = self._index(state)\n",
    "        return np.sum(self._n[idx]) if action is None else self._n[idx][action]\n",
    "\n",
    "    def _preprocess_state(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess the state to be used in the tabular Q function.\"\"\"\n",
    "        if not np.issubdtype(state.dtype, np.integer):\n",
    "            state = self._discretize(state)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _index(self, state: np.ndarray, action: Optional[int] = None) -> tuple:\n",
    "        \"\"\"Get the index of a state or a state-action pair.\n",
    "\n",
    "        It is solved according to the injective mapping of a state the index\n",
    "        in the Q table.\n",
    "\n",
    "        :param state: state to be evaluated.\n",
    "        :param action: action to be evaluated. If None, returns the index of the\n",
    "         state.\n",
    "        :return: index of a state or a state-action pair.\n",
    "        \"\"\"\n",
    "        state_idx = state + self.discrete_scale\n",
    "        if any(state_idx < 0):\n",
    "            raise ValueError(\"Index is negative\")\n",
    "        if action is None:\n",
    "            idx = tuple(state_idx)\n",
    "        else:\n",
    "            idx = tuple(state_idx) + (action,)\n",
    "        return idx\n",
    "\n",
    "    def _discretize(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Discretize the state.\n",
    "\n",
    "        :param state: state to be discretized. It must be a numpy array.\n",
    "        \"\"\"\n",
    "        state = (state * self.discrete_scale).astype(int)\n",
    "        return state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d96ae60f86aa905e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Linear Q-function\n",
    "\n",
    "The linear Q-function is a linear approximation of the tabular Q-function. It is defined as $Q(s) = W^T x(s)$, where $W$ is the weight vector mapping the state space to the action space and $x(s)$ is the input sample from the state space.\n",
    "\n",
    "The code below has a QAbstractApproximation since it is used as a base class for the linear Q-function and the DQN Q-function. Which we are going to implement later.\n",
    "\n",
    "It is important noting that even a Linear Q-function wraps a tabular Q-function, since it is important to store the number of times a state-action pair was visited. This is necessary in several occasions that we will face throughout this report."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f91dca98d034085"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ABC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mQAbstractApproximation\u001B[39;00m(\u001B[43mABC\u001B[49m):\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Abstract class for Q function approximations.\"\"\"\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;28mself\u001B[39m, n_actions: \u001B[38;5;28mint\u001B[39m, n_feat: \u001B[38;5;28mint\u001B[39m, discrete_scale: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m40\u001B[39m\n\u001B[1;32m      6\u001B[0m     ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ABC' is not defined"
     ]
    }
   ],
   "source": [
    "class QAbstractApproximation(ABC):\n",
    "    \"\"\"Abstract class for Q function approximations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_actions: int, n_feat: int, discrete_scale: int = 40\n",
    "    ) -> None:\n",
    "        \"\"\"Define generic Q function approximation.\n",
    "\n",
    "        It includes a tabular Q function in cases where discrete evaluation is\n",
    "        necessary.\n",
    "\n",
    "        :param n_actions: number of actions available in the environment.\n",
    "        :param n_feat: number of features in the state.\n",
    "        :param discrete_scale: scale in which the state will be discretized. The\n",
    "         state will be multiplied by this number and then rounded to the nearest\n",
    "         integer.\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.n_feat = n_feat\n",
    "        self.q_tabular = QTabular(n_actions, n_feat, discrete_scale)\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(\n",
    "        self, state: np.ndarray, action: Optional[int] = None\n",
    "    ) -> Union[np.ndarray, torch.Tensor]:\n",
    "        \"\"\"Call function to get the Q value of a state-action pair.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def q_max(self, state: np.ndarray) -> tuple[int, float]:\n",
    "        \"\"\"Maximal Q value for a given state.\n",
    "\n",
    "        Get the action and Q value for the maximal Q value for a given state.\n",
    "\n",
    "        :param state: state to be evaluated.\n",
    "        :return: action and Q value for the maximal Q value for a given state.\n",
    "        \"\"\"\n",
    "        values = self(state)\n",
    "        if isinstance(values, torch.Tensor):\n",
    "            values = values.detach().cpu().numpy().astype(np.float32)\n",
    "        maximal_value = values.max()\n",
    "        maximal_set = np.argwhere(values == maximal_value).flatten()\n",
    "        action = np.random.choice(maximal_set)\n",
    "\n",
    "        return action, maximal_value.item()\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        \"\"\"Get missing attributes from the tabular Q function.\"\"\"\n",
    "        return getattr(self.q_tabular, name)\n",
    "\n",
    "\n",
    "class QLinear(QAbstractApproximation):\n",
    "    \"\"\"Linear Q function value approximation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_lr: float = 0.0001, *args: Any, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Define a linear Q function approximation.\n",
    "\n",
    "        It is an unbiased linear approximation of the Q function. It is defined\n",
    "        as Q(s) = W^T x(s), where W is the weight vector mapping the state\n",
    "        space to the action space and x(s) is the input sample from the state\n",
    "        space.\n",
    "\n",
    "        :param base_lr: base learning rate to scale the learning rate of the\n",
    "         control algorithm.\n",
    "        :param args: arguments to be passed to the QAbstractApproximation.\n",
    "        :param kwargs: keyword arguments to be passed to the\n",
    "         QAbstractApproximation.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.base_lr = base_lr\n",
    "        self.weights = np.zeros((self.n_feat, self.n_actions))\n",
    "\n",
    "    def __call__(\n",
    "        self, state: np.ndarray, action: Optional[int] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Call function to get the Q value of a state-action pair.\"\"\"\n",
    "        x = np.asarray(state)\n",
    "        y = self.weights.T @ x\n",
    "        return y[action] if action is not None else y\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        expected: float,\n",
    "        _: float,\n",
    "        α: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Update the Q linear approximation for a given state-action pair.\n",
    "\n",
    "        It is a classical update rule for a linear Q function approximation. The\n",
    "        update rule follows the principle of minimizing the error between the\n",
    "        expected and the predicted value.\n",
    "\n",
    "        :param state: state associated.\n",
    "        :param action: action associated.\n",
    "        :param expected: expected value for the evaluated policy.\n",
    "        :param _: ignored\n",
    "        :param α: learning rate.\n",
    "        \"\"\"\n",
    "        predicted = self(state, action).item()\n",
    "        self.q_tabular.update(state, action, expected, predicted, α)\n",
    "        α *= self.base_lr\n",
    "\n",
    "        self.weights[:, action] += (\n",
    "            α * (expected - predicted) * np.asarray(state)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:35:00.416206217Z",
     "start_time": "2023-12-19T15:35:00.286299866Z"
    }
   },
   "id": "98c1977a89f237bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agent definition\n",
    "\n",
    "The agent is just a wrapper for the Q-function. It is used to choose an action based on the current state. The action is chosen using an epsilon-greedy strategy.\n",
    "\n",
    "Mainly there are two epsilon-greedy strategies. The first is the one used in the DQN paper, which is defined as $\\epsilon = 0.05 + 0.85 * e^{-steps\\_done / 1000}$. The second is the one requested by the final project specification, which is defined as $\\epsilon = N_0 / (N_0 + N(s))$, where $N_0$ is a hyperparameter and $N(s)$ is the number of times the state $s$ has been visited.\n",
    "\n",
    "It also has defined a stochasticity factor, which is a hyperparameter that defines the stochasticity of the agent. The action is chosen randomly with probability equal to the stochasticity factor. Otherwise, the action is chosen according to the epsilon-greedy strategy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b1c75cd05f353a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent class based on a ϵ-greedy policy.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        q_function: Union[QTabular, QAbstractApproximation],\n",
    "        n_actions: int,\n",
    "        eps_greedy_function: str = \"dqn\",\n",
    "        n0: Optional[float] = None,\n",
    "        stochasticity_factor: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"Agent class.\n",
    "\n",
    "        Define an agent that chooses an action based on the current state.\n",
    "        The action is chosen using an epsilon-greedy strategy.\n",
    "\n",
    "        :param q_function: Q function used to choose the action.\n",
    "        :param n0: parameter used in the epsilon-greedy function.\n",
    "        :param n_actions: number of actions available in the environment.\n",
    "        :param eps_greedy_function: string that defines the epsilon-greedy\n",
    "         function. The options are:\n",
    "         - \"s\": epsilon is a function of the number of times the state has\n",
    "         been visited. Defined as N_0 / (N_0 + N(s)), where N(s) is the number\n",
    "         of times the state s has been visited.\n",
    "         - \"t\": epsilon is a function of the number of episodes. Defined as\n",
    "         N_0 / (N_0 + t), where t is the current episode.\n",
    "         - \"dqn\": epsilon is a function of the  number of steps as used in\n",
    "         the DQN paper. Defined as 0.05 + 0.85 * exp(-steps_done / 1000).\n",
    "        :param stochasticity_factor: factor that defines the stochasticity of\n",
    "         the agent. The action is chosen randomly with probability equal to\n",
    "         stochasticity_factor.\n",
    "        \"\"\"\n",
    "        self.q_function = q_function\n",
    "        self.stochasticity_factor = stochasticity_factor\n",
    "\n",
    "        self.ϵ: Callable[[np.ndarray, int], float]\n",
    "\n",
    "        if eps_greedy_function == \"dqn\" and n0 is not None:\n",
    "            raise ValueError(\n",
    "                \"N0 is not defined when using the DQN epsilon function\"\n",
    "            )\n",
    "        elif eps_greedy_function != \"dqn\" and n0 is None:\n",
    "            raise ValueError(\n",
    "                \"N0 must be defined when using a custom epsilon function\"\n",
    "            )\n",
    "\n",
    "        if eps_greedy_function == \"s\":\n",
    "            self.ϵ = lambda s, t: n0 / (n0 + self.q_function.n(s))  # type: ignore\n",
    "        elif eps_greedy_function == \"t\":\n",
    "            self.ϵ = lambda s, t: n0 / (n0 + t)  # type: ignore\n",
    "        elif eps_greedy_function == \"dqn\":\n",
    "            self.ϵ = lambda *_: 0.05 + 0.85 * np.exp(  # type: ignore\n",
    "                -self.steps_done / 1000\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown epsilon function\")\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def act(self, state: np.ndarray, current_episode: int) -> int:\n",
    "        \"\"\"Agent action.\n",
    "\n",
    "        Choose an action based on the current state.\n",
    "        The action is chosen using an epsilon-greedy strategy.\n",
    "\n",
    "        :param state:\n",
    "            current state.\n",
    "        :param current_episode:\n",
    "            current episode.\n",
    "        :return:\n",
    "            action chosen.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            action, best_reward = self.q_function.q_max(state)\n",
    "\n",
    "        self.steps_done += 1\n",
    "        # ϵ-greedy strategy to choose the action\n",
    "        t = np.random.uniform()\n",
    "        s = np.random.uniform()\n",
    "        if t > self.ϵ(state, current_episode) and s > self.stochasticity_factor:\n",
    "            return action\n",
    "        else:\n",
    "            return np.random.randint(self.n_actions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34e07fbd5212f5bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replay memory\n",
    "\n",
    "Here, I am going to define the replay memory. It is used to store the transitions of the agent. It is used to break the correlation between the samples, which is important for the learning process. It is also used to store the transitions of the agent, which is useful for the DQN algorithm.\n",
    "\n",
    "Although not required in the project specification, it can be used to store the transitions of the agent in the other algorithms. Such as Q-Learning, Monte-Carlo and Sarsa-Lambda. However, I do not plan to use it in these algorithms. I will use it only in the DQN algorithm. To ensure generalization, I am going to use those algorithms with a batch size valued None, which means that the algorithm will consume and release the transitions in the replay memory as soon as they are available. This way, the replay memory will not be used in these algorithms."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5e59d333c018298"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from random import sample\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"Replay memory as described in the DQN paper.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, batch_size: Optional[int]) -> None:\n",
    "        \"\"\"Initialize the replay memory.\n",
    "\n",
    "        :param capacity: The maximum number of transitions to be saved.\n",
    "        :param batch_size: The batch size to be sampled.\n",
    "        \"\"\"\n",
    "        self.memory: deque = deque([], maxlen=capacity)\n",
    "        if batch_size is None:\n",
    "            self.consume_and_release = True\n",
    "            self.batch_size = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    def push(self, *args: Any) -> None:\n",
    "        \"\"\"Save a transition.\n",
    "\n",
    "        :param args: The transition to be saved.\n",
    "        \"\"\"\n",
    "        self.memory.append(args)\n",
    "\n",
    "    @property\n",
    "    def batch_ready(self) -> bool:\n",
    "        \"\"\"Whether the batch is ready.\n",
    "\n",
    "        :return: Whether the batch is ready.\n",
    "        \"\"\"\n",
    "        return len(self.memory) >= self.batch_size\n",
    "\n",
    "    def sample(self) -> list:\n",
    "        \"\"\"Sample a batch according to the batch size.\n",
    "\n",
    "        :return: A batch of transitions.\n",
    "        \"\"\"\n",
    "        if not self.batch_ready:\n",
    "            raise RuntimeError(\n",
    "                \"The batch is not ready. There are not enough \" \"transitions.\"\n",
    "            )\n",
    "        if self.consume_and_release:\n",
    "            ret = [self.memory.popleft() for _ in range(self.batch_size)]\n",
    "            return ret\n",
    "        else:\n",
    "            return sample(self.memory, self.batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:40:16.890116431Z",
     "start_time": "2023-12-19T15:40:16.790049708Z"
    }
   },
   "id": "38bf96495c53f7f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Control algorithms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60fbbd4fcbca97e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Abstract Control\n",
    "\n",
    "It wraps most of the control logic algorithm and defines the interface for the control algorithms. It is used as a base class for the control algorithms.\n",
    "\n",
    "The main components of the control algorithm are:\n",
    "- `fit`: the main loop of the control algorithm. It iterates over the episodes\n",
    "  and the steps of each episode. It calls the `update_on_step` and\n",
    "  `update_on_episode_end` methods, which can be implemented by the children\n",
    "  classes.\n",
    "- `update_on_step`: update the agent on a step. It is called on each step of\n",
    "    the episode. It is used by the Q-Learning, and Sarsa-Lambda.\n",
    "- `update_on_episode_end`: update the agent on an episode end. It is called on\n",
    "    each episode end. It is used by the Monte-Carlo.\n",
    "- `reset`: reset the control algorithm. It is called on each episode. It is useful\n",
    "    if the control algorithm needs to reset some internal state. It is used by\n",
    "    the Sarsa-Lambda.\n",
    "- `optimize`: optimize the Q-function. It can be called by the children classes\n",
    "    to optimize the Q-function. However, when the function is different from \n",
    "    the conceptual update rule adopted by the control algorithm, it must be \n",
    "    overridden by the children classes. Such as the case of the Sarsa-Lambda."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c99e45a92963291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AbstractControl(ABC):\n",
    "    \"\"\"Abstract class for control algorithms.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        agent: Agent,\n",
    "        gamma: float,\n",
    "        replay_capacity: int = 10000,\n",
    "        num_episodes: int = 1000,\n",
    "        batch_size: int = 32,\n",
    "        reward_mode: str = \"default\",\n",
    "        verbose: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Default constructor for the AbstractControl class.\n",
    "\n",
    "        :param env: The gym environment.\n",
    "        :param agent: The agent.\n",
    "        :param replay_capacity: The replay capacity for the replay memory.\n",
    "        :param num_episodes: The number of episodes in which the algorithm will\n",
    "         learn.\n",
    "        :param gamma: The discount factor.\n",
    "        :param batch_size: The batch size for the replay memory.\n",
    "        :param reward_mode: The reward mode. It can be 'default' or 'sparse'. In\n",
    "         the 'default' mode, the reward is the reward received from the\n",
    "         environment. In the 'sparse' mode, the reward is the total reward\n",
    "         received in the episode.\n",
    "        :param verbose: Whether to print the progress bar.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.verbose = verbose\n",
    "        self.agent = agent\n",
    "        self.num_episodes = num_episodes\n",
    "        self.q_function = agent.q_function\n",
    "        self.γ = gamma\n",
    "        self.α_t: Callable[[np.ndarray, int], float] = (\n",
    "            lambda s, a: 1 / self.q_function.n(s, a)\n",
    "            if self.q_function.n(s, a) > 0\n",
    "            else 1\n",
    "        )\n",
    "        self.memory = ReplayMemory(replay_capacity, batch_size)\n",
    "\n",
    "        self.reward_processor: Callable[[float, bool, float], float]\n",
    "        if reward_mode == \"default\":\n",
    "            self.reward_processor = lambda r, *_: r\n",
    "        elif reward_mode == \"sparse\":\n",
    "            self.reward_processor = (\n",
    "                lambda _, done, total_reward: total_reward if done else 0\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid reward mode. Expected 'default' or 'sparse', got \"\n",
    "                f\"{reward_mode}\"\n",
    "            )\n",
    "\n",
    "    def fit(self) -> list[float]:\n",
    "        \"\"\"Fitting loop for the control algorithm.\n",
    "\n",
    "        It is built according to a generic reinforcement learning loop that\n",
    "        iterates over the episodes and the steps of each episode. The children\n",
    "        classes must implement the update_on_step and update_on_episode_end\n",
    "        methods, in which they can control how the agent learns.\n",
    "\n",
    "        :return: The mean of the last 10% of the episodes.\n",
    "        \"\"\"\n",
    "        episodes_rewards = []\n",
    "\n",
    "        pbar = tqdm(range(self.num_episodes), disable=not self.verbose)\n",
    "        for i_episode in pbar:\n",
    "            state, _ = self.env.reset(seed=i_episode)\n",
    "            action = self.agent.act(state, current_episode=i_episode)\n",
    "            returns = []\n",
    "            total_reward = 0.0\n",
    "            self.reset()\n",
    "            while True:\n",
    "                state_prime, reward, done, truncated, info = self.env.step(\n",
    "                    action\n",
    "                )\n",
    "                reward = float(reward)\n",
    "                total_reward += reward\n",
    "                reward_processed = self.reward_processor(\n",
    "                    reward, done, total_reward\n",
    "                )\n",
    "                returns.append((state, action, reward_processed))\n",
    "\n",
    "                action_prime = self.agent.act(\n",
    "                    state_prime, current_episode=i_episode\n",
    "                )\n",
    "                loss_updated = self.update_on_step(\n",
    "                    state,\n",
    "                    action,\n",
    "                    reward_processed,\n",
    "                    state_prime if not done else None,\n",
    "                    action_prime,\n",
    "                    done,\n",
    "                )\n",
    "                if loss_updated is not None:\n",
    "                    pbar.set_postfix(loss=loss_updated)\n",
    "\n",
    "                if done or truncated:\n",
    "                    loss_updated = self.update_on_episode_end(returns)\n",
    "                    if loss_updated is not None:\n",
    "                        pbar.set_postfix(loss=loss_updated)\n",
    "\n",
    "                    episodes_rewards.append(total_reward)\n",
    "                    moving_average = np.mean(episodes_rewards[-100:])\n",
    "\n",
    "                    pbar.set_description(\n",
    "                        f\"Total reward for episode {i_episode:3d}: \"\n",
    "                        f\"{total_reward}, moving average: {moving_average:.2f},\"\n",
    "                        f\" states explored: {self.q_function.states_explored}\",\n",
    "                        refresh=False,\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                state = state_prime\n",
    "                action = action_prime\n",
    "\n",
    "        return episodes_rewards\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_on_step(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        state_prime: np.ndarray,\n",
    "        action_prime: int,\n",
    "        done: bool,\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"Abstract method to update the agent on a step.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_on_episode_end(self, returns: list) -> Optional[float]:\n",
    "        \"\"\"Abstract method to update the agent on an episode end.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self) -> None:  # noqa: B027\n",
    "        \"\"\"Reset the control algorithm.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimize(self) -> Optional[float]:\n",
    "        \"\"\"Generic optimization of the policy network.\n",
    "\n",
    "        It follows the rule that generally the optimization goal is designed\n",
    "        to minimize the difference between the expected and the predicted\n",
    "        value. It is performed on a batch of samples from the replay memory.\n",
    "\n",
    "        :return: The loss of the optimization step. If no optimization step was\n",
    "         performed, returns None.\n",
    "        \"\"\"\n",
    "        if self.memory.batch_ready:\n",
    "            batch = self.memory.sample()\n",
    "            loss = 0\n",
    "            updated_batch = []\n",
    "            for b in batch:\n",
    "                s, a, y_callable, _, α = b\n",
    "                if isinstance(y_callable, Callable):  # type: ignore\n",
    "                    y = y_callable()\n",
    "                else:\n",
    "                    y = y_callable\n",
    "                updated_batch.append((s, a, y, _, α))\n",
    "\n",
    "            for ub in updated_batch:\n",
    "                _loss = self.q_function.update(*ub)\n",
    "                if _loss is not None:\n",
    "                    loss += _loss\n",
    "            return loss / len(batch)\n",
    "        else:\n",
    "            return None\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe69ff4dd4b4b932"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Monte-Carlo Control\n",
    "\n",
    "The Monte-Carlo control algorithm is implemented as a subclass of the AbstractControl class. It implements the update_on_episode_end method, which is used to update the agent on an episode end. \n",
    "\n",
    "The target learning is to predict $G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-1} R_T$, where $T$ is the last time step of the episode. \n",
    "\n",
    "The update rule is defined as $Q(S_t, A_t) = Q(S_t, A_t) + \\alpha (G_t - Q(S_t, A_t))$, where $\\alpha$ is the learning rate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76751d383c1577b6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MonteCarloControl(AbstractControl):\n",
    "    \"\"\"Monte Carlo control algorithm implementation.\"\"\"\n",
    "\n",
    "    def update_on_step(self, *_: Any) -> Optional[float]:\n",
    "        \"\"\"Dummy method to avoid abstraction unimplemented error.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_on_episode_end(self, returns: list) -> Optional[float]:\n",
    "        \"\"\"Feedback the agent with the returns.\"\"\"\n",
    "        gt = 0\n",
    "        count = 0\n",
    "        for state, action, reward in reversed(returns):\n",
    "            gt = self.γ * gt + reward\n",
    "\n",
    "            # Update the mean for the action-value function Q(s,a)\n",
    "            self.memory.push(state, action, gt, None, self.α_t(state, action))\n",
    "            count += 1\n",
    "\n",
    "        if self.memory.consume_and_release:\n",
    "            # workaround a.k.a gambiarra\n",
    "            old_batch_size = self.memory.batch_size\n",
    "            self.memory.batch_size = count\n",
    "            ret = self.optimize()\n",
    "            self.memory.batch_size = old_batch_size\n",
    "            return ret\n",
    "        else:\n",
    "            return self.optimize()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T17:09:51.023086616Z",
     "start_time": "2023-12-19T17:09:50.982628249Z"
    }
   },
   "id": "5dc1dfef2da4ff53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Q-Learning Control\n",
    "\n",
    "The Q-Learning control algorithm is implemented as a subclass of the AbstractControl class. It implements the update_on_step method, which is used to update the agent on a step.\n",
    "\n",
    "The target learning is to predict $R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)$. The expected value is evaluated based on the current policy, which can vary over time. Hence, does not make sense to store the expected value in the replay memory. Instead, we store a callable that returns the expected value. This way, we can evaluate the expected value at the time of the optimization step."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff71e7518f68eb9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "class QLearningControl(AbstractControl):\n",
    "    \"\"\"Q-Learning control algorithm implementation.\"\"\"\n",
    "\n",
    "    def update_on_step(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        _: Any,\n",
    "        done: bool,\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"Feedback the agent with the transitions on a determined step.\n",
    "\n",
    "        :param state: The current state\n",
    "        :param action: The action taken\n",
    "        :param reward: The reward received\n",
    "        :param next_state: The next state\n",
    "        :param _: ignored\n",
    "        :param done: Whether the episode is over\n",
    "\n",
    "        :return: The loss of the optimization step. If no optimization step was\n",
    "         performed, returns None.\n",
    "        \"\"\"\n",
    "        # Compute the max Q(s',a')\n",
    "        max_q = partial(self.q_function.q_max, next_state)\n",
    "\n",
    "        # Compute a dynamic expected value according to the done flag and the\n",
    "        # max Q(s',a')\n",
    "        def expected(\n",
    "            r: float, done: bool, max_q: Callable\n",
    "        ) -> Callable[[], float]:\n",
    "            return lambda: r + self.γ * max_q()[1] if not done else r\n",
    "\n",
    "        self.memory.push(\n",
    "            state,\n",
    "            action,\n",
    "            expected(reward, done, max_q),\n",
    "            None,\n",
    "            self.α_t(state, action),\n",
    "        )\n",
    "\n",
    "        return self.optimize()\n",
    "\n",
    "    def update_on_episode_end(self, *_: Any) -> None:\n",
    "        \"\"\"Dummy method to avoid abstraction unimplemented error.\"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e357849cf38d0d1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sarsa-Lambda Control\n",
    "\n",
    "Sarsa-lambda is tricky.\n",
    "\n",
    "The tabular version of the algorithm is implemented as described in the book. The linear version is the \"true online sarsa($\\lambda$)\" implemented as described in the book \"Reinforcement Learning: An Introduction\" by Sutton and Barto."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ddd31c36c9556f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _sarsa_update_q_function(n: np.ndarray, q: np.ndarray, etrace: dict, α: float, δ: float, γ: float, λ: float) -> None:\n",
    "    \"\"\"Update the Q function for the Sarsa algorithm.\"\"\"\n",
    "    for i, e in etrace.items():\n",
    "        n[i] += α * e\n",
    "        q[i] += α * δ * e\n",
    "        etrace[i] *= γ * λ\n",
    "\n",
    "\n",
    "class SarsaLambdaControl(AbstractControl):\n",
    "    \"\"\"Sarsa control algorithm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, lambda_factor: float, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Constructor for the Sarsa control algorithm.\n",
    "\n",
    "        :param lambda_factor: The lambda factor for the eligibility trace.\n",
    "        :param args: remaining arguments from :AbstractControl:\n",
    "         `rl_final_project.AbstractControl.__init__`\n",
    "        :param kwargs: remaining keyword arguments\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.λ = lambda_factor\n",
    "        self.e = numba.typed.Dict()\n",
    "        self.q_old = 0\n",
    "        self.z = np.zeros(self.q_function.n_feat)\n",
    "\n",
    "    def update_on_step(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        next_action: int,\n",
    "        done: bool,\n",
    "    ) -> None:\n",
    "        \"\"\"Feedback the agent with the returns.\"\"\"\n",
    "\n",
    "        def q_prime_instantiation(\n",
    "            next_state: np.ndarray, next_action: int\n",
    "        ) -> Callable:\n",
    "            return (\n",
    "                lambda: self.q_function(next_state, next_action)\n",
    "                if next_state is not None\n",
    "                else 0\n",
    "            )\n",
    "\n",
    "        q_prime = q_prime_instantiation(next_state, next_action)\n",
    "\n",
    "        def q_instantiation(\n",
    "            s: np.ndarray, a: int\n",
    "        ) -> Callable[[], Union[float, np.ndarray, Tensor]]:\n",
    "            return lambda: self.q_function(s, a)\n",
    "\n",
    "        q = q_instantiation(state, action)\n",
    "\n",
    "        def δ_instantiation(\n",
    "            r: float, q_prime_call: Callable, q_call: Callable\n",
    "        ) -> Callable[[], float]:\n",
    "            return lambda: r + self.γ * q_prime_call() - q_call()\n",
    "\n",
    "        δ = δ_instantiation(reward, q_prime, q)\n",
    "\n",
    "        s = self.q_function._preprocess_state(state)\n",
    "        idx = self.q_function._index(s, action)\n",
    "        α = self.α_t(state, action)\n",
    "\n",
    "        # self.memory.push(S, A, q_prime, q, δ, self.α_t(S, A))\n",
    "\n",
    "        if isinstance(self.q_function, QLinear):\n",
    "            x = np.asarray(state)\n",
    "            self.z = (\n",
    "                self.γ * self.λ * self.z\n",
    "                + (1 - α * self.γ * self.λ * self.z.T @ x) * x\n",
    "            )\n",
    "            self.q_function.weights[:, action] += (\n",
    "                α * 0.1 * (δ() + q() - self.q_old) * self.z\n",
    "                - α * (q() - self.q_old) * x\n",
    "            )\n",
    "            self.q_function.q_tabular._n[idx] += 1\n",
    "            self.q_function.q_tabular.count_non_zero += 1\n",
    "            self.q_old = q_prime()\n",
    "        elif isinstance(self.q_function, QTabular):\n",
    "            self.memory.push(\n",
    "                state, action, q_prime, q, δ, self.α_t(state, action)\n",
    "            )\n",
    "            self.optimize()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        \"\"\"Optimize the policy network.\n",
    "\n",
    "        It is applied the tabular version of the algorithm.\n",
    "        \"\"\"\n",
    "        if isinstance(self.q_function, QTabular):\n",
    "            if self.memory.batch_ready:\n",
    "                batch = self.memory.sample()\n",
    "                updated_batch = []\n",
    "                for b in batch:\n",
    "                    s, a, q_prime, q, δ, α = b\n",
    "                    updated_batch.append((s, a, q_prime(), q(), δ(), α))\n",
    "\n",
    "                for ub in updated_batch:\n",
    "                    s, a, q_prime, q, δ, α = ub\n",
    "                    s = self.q_function._preprocess_state(s)\n",
    "                    idx = self.q_function._index(s, a)\n",
    "\n",
    "                    if idx not in self.e:\n",
    "                        self.e[idx] = 0.0\n",
    "                        self.q_function.count_non_zero += 1\n",
    "\n",
    "                    # compiled code for fast update\n",
    "                    _sarsa_update_q_function(self.q_function._n, self.q_function._q, self.e, α, δ, self.γ, self.λ)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def update_on_episode_end(self, *_: Any) -> None:\n",
    "        \"\"\"Dummy method to avoid abstraction unimplemented error.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the control algorithm.\"\"\"\n",
    "        self.q_old = 0\n",
    "        self.z = np.zeros(self.q_function.n_feat)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e6a1fed24a9fdd4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN algorithm\n",
    "\n",
    "I implemented the DQN algorithm in the framework of the control algorithms. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55220bf1174ad74a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from rl_final_project.control import AbstractControl\n",
    "from rl_final_project.q_functions import QAbstractApproximation\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q Network Torch Module.\"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int) -> None:\n",
    "        \"\"\"Deep Q Network.\n",
    "\n",
    "        Implements a simple MLP with 2 hidden layers with 128 neurons each.\n",
    "\n",
    "        :param n_observations: Number of observations\n",
    "        :param n_actions: Number of actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "\n",
    "class DQNFunction(QAbstractApproximation):\n",
    "    \"\"\"Deep Q Network Q Function Approximation.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Deep Q Network Q Function Approximation.\n",
    "\n",
    "        :param batch_size: Batch size\n",
    "        :param args: Remaining arguments for the QAbstractApproximation\n",
    "        :param kwargs: Remaining keyword arguments for the\n",
    "         QAbstractApproximation\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.policy = DQN(self.n_feat, self.n_actions).to(self.device)\n",
    "        self.target = DQN(self.n_feat, self.n_actions).to(self.device)\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def __call__(\n",
    "        self, state: np.ndarray, action: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        It converts the state to a torch tensor and passes it to the network.\n",
    "        If an action is provided, it returns the Q value for that action.\n",
    "\n",
    "        :param state: The state to evaluate\n",
    "        :param action: The action to evaluate\n",
    "        :return: The Q value for the state-action pair or the Q values for all\n",
    "         actions if no action is provided\n",
    "        \"\"\"\n",
    "        state_tsr = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        if action is not None:\n",
    "            return self.policy(state_tsr)[action]\n",
    "        else:\n",
    "            return self.policy(state_tsr)\n",
    "\n",
    "\n",
    "class DQNControl(AbstractControl):\n",
    "    \"\"\"DQN Control algorithm.\n",
    "\n",
    "    Implements the DQN algorithm as described in the paper:\n",
    "    https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, lr: float, tau: float, *args: Any, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor for the DQN control algorithm.\n",
    "\n",
    "        :param lr: Learning rate\n",
    "        :param tau: Soft update parameter\n",
    "        :param args: remaining arguments\n",
    "        :param kwargs: remaining keyword arguments\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if not isinstance(self.q_function, DQNFunction):\n",
    "            raise ValueError(\"Q function must be a DQN\")\n",
    "        self.device = self.q_function.device\n",
    "        self.policy = self.q_function.policy\n",
    "        self.target = self.q_function.target\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.policy.parameters(), lr=lr, amsgrad=True\n",
    "        )\n",
    "        self.τ = tau\n",
    "        self.criteria = nn.SmoothL1Loss()\n",
    "\n",
    "    def update_on_episode_end(self, returns: list) -> Optional[float]:\n",
    "        \"\"\"Feedback the agent with the returns.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_on_step(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        next_action: int,\n",
    "        done: bool,\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"Feedback the agent with the transitions on a determined step.\n",
    "\n",
    "        :param state: The current state\n",
    "        :param action: The action taken\n",
    "        :param reward: The reward received\n",
    "        :param next_state: The next state\n",
    "        :param next_action: The next action\n",
    "        :param done: Whether the episode is over\n",
    "        :return: The loss of the optimization step.\n",
    "        \"\"\"\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        loss = self.optimize()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = self.target.state_dict()\n",
    "        policy_net_state_dict = self.policy.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[\n",
    "                key\n",
    "            ] * self.τ + target_net_state_dict[key] * (1 - self.τ)\n",
    "        self.target.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def optimize(self) -> Optional[float]:\n",
    "        \"\"\"Optimize the policy network.\n",
    "\n",
    "        :return: The loss of the optimization step. If no optimization step was\n",
    "         performed, returns None.\n",
    "        \"\"\"\n",
    "        if self.memory.batch_ready:\n",
    "            batch = self.memory.sample()\n",
    "\n",
    "            s_lst = []\n",
    "            a_lst = []\n",
    "            r_lst = []\n",
    "            s_next_lst = []\n",
    "            for b in batch:\n",
    "                sb, sa, sb_prime, rb = b\n",
    "                s_lst.append(sb)\n",
    "                a_lst.append(sa)\n",
    "                r_lst.append(rb)\n",
    "                s_next_lst.append(sb_prime)\n",
    "\n",
    "            s = torch.tensor(s_lst, dtype=torch.float32, device=self.device)\n",
    "            a = torch.tensor(\n",
    "                a_lst, dtype=torch.int64, device=self.device\n",
    "            ).unsqueeze(0)\n",
    "            r = torch.tensor(r_lst, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            all_indices = torch.arange(\n",
    "                self.memory.batch_size, device=self.device\n",
    "            )\n",
    "            state_action_values = (\n",
    "                self.policy(s)[all_indices, a].squeeze(0).unsqueeze(1)\n",
    "            )\n",
    "\n",
    "            non_final_mask = torch.tensor(\n",
    "                [False if s is None else True for s in s_next_lst],\n",
    "                device=self.device,\n",
    "                dtype=torch.bool,\n",
    "            )\n",
    "            non_final_next_states = torch.tensor(\n",
    "                [s for s in s_next_lst if s is not None]\n",
    "            ).to(self.device)\n",
    "            next_state_values = torch.zeros(\n",
    "                self.memory.batch_size, device=self.device\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                next_state_values[non_final_mask] = (\n",
    "                    self.target(non_final_next_states).max(1).values\n",
    "                )\n",
    "\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = (next_state_values * self.γ) + r\n",
    "            loss = self.criteria(\n",
    "                state_action_values, expected_state_action_values.unsqueeze(1)\n",
    "            )\n",
    "\n",
    "            # Optimize the model\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # In-place gradient clipping\n",
    "            torch.nn.utils.clip_grad_value_(self.policy.parameters(), 100)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            return loss.detach().item()\n",
    "\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "711c5e4ee2eba5c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simple run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5481838203bd9689"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a9d6262c76039700"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "\n",
    "In this section, I am going to analyze the results of the experiments. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84af3c6722316e47"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T14:05:43.521919280Z"
    }
   },
   "id": "b640611deffbcb56"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "cmap = {\n",
    "    \"sarsa-lambda\": SarsaLambdaControl,\n",
    "    \"q-learning\": QLearningControl,\n",
    "    \"dqn\": DQNControl,\n",
    "    \"monte-carlo\": MonteCarloControl,\n",
    "}\n",
    "\n",
    "fmap = {\n",
    "    \"linear\": QLinear,\n",
    "    \"tabular\": QTabular,\n",
    "}\n",
    "\n",
    "def build_control_experiment(\n",
    "        env: gym.Env,\n",
    "        method: str, \n",
    "        gamma: float,\n",
    "        function: Optional[str] = None,\n",
    "        num_episodes: int = 10_000,\n",
    "        replay_capacity: int = 10_000,\n",
    "        n0: int = 10,\n",
    "        discrete_scale: int = 1,\n",
    "        batch_size: int = 128,\n",
    "        eps_func: str = \"dqn\",\n",
    "        stochasticity_factor: float = 0.4,\n",
    "        method_args: Optional[dict] = None,\n",
    ") -> AbstractControl:\n",
    "    \n",
    "    if method_args is None:\n",
    "        method_args = {}\n",
    "    \n",
    "    if function is None and method != \"dqn\":\n",
    "        raise ValueError(\"function must be specified for all methods except dqn\")\n",
    "    \n",
    "    if function is not None and function not in fmap:\n",
    "        raise ValueError(f\"Unknown function approximation {function}\")\n",
    "    \n",
    "    if method not in cmap:\n",
    "        raise ValueError(f\"Unknown control method {method}\")\n",
    "    \n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    \n",
    "    if method == \"dqn\":       \n",
    "        q_function = DQNFunction(\n",
    "            batch_size=batch_size,\n",
    "            n_actions=n_actions,\n",
    "            n_feat=n_states,\n",
    "            discrete_scale=discrete_scale,\n",
    "        )\n",
    "    else:\n",
    "        q_function = fmap[function](\n",
    "            n_actions=n_actions,\n",
    "            n_feat=n_states,\n",
    "            discrete_scale=discrete_scale\n",
    "        )\n",
    "        \n",
    "    agent = Agent(\n",
    "        q_function, \n",
    "        n0=n0, \n",
    "        n_actions=n_actions, \n",
    "        eps_greedy_function=eps_func,\n",
    "        stochasticity_factor=stochasticity_factor,\n",
    "    )\n",
    "    \n",
    "    for k in copy(method_args):\n",
    "        if method_args[k] is None:\n",
    "            del method_args[k]\n",
    "    \n",
    "    control = cmap[method](\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=num_episodes,\n",
    "        gamma=gamma,\n",
    "        batch_size=batch_size,\n",
    "        replay_capacity=replay_capacity,\n",
    "        verbose=False,\n",
    "        **method_args\n",
    "    )\n",
    "    \n",
    "    return control\n",
    "        \n",
    "exp_config = SimpleNamespace(\n",
    "    env_name=\"CartPole-v0\",\n",
    "    num_episodes=10_000,\n",
    "    control_algorithm=SimpleNamespace(\n",
    "        method=\"sarsa\", \n",
    "        args=SimpleNamespace(\n",
    "            λ=0.8\n",
    "        ),\n",
    "        function=\"linear\",\n",
    "    ),\n",
    "    gamma=0.9,\n",
    "    discrete_scale=10,\n",
    "    n0=10,\n",
    "    eps_func=\"dqn\",\n",
    "    stochasticity_factor=0.4,\n",
    "    reward_mode=\"normal\",\n",
    "    seed=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T14:05:44.289593360Z",
     "start_time": "2023-12-19T14:05:44.288204415Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from rl_final_project.environment import EnvironmentNormalizer\n",
    "\n",
    "\n",
    "class ExperimentExitCode:\n",
    "    SUCCESS = 0\n",
    "    FAILED = 1\n",
    "    INVALID = 2\n",
    "\n",
    "def run(config: SimpleNamespace) -> tuple[ExperimentExitCode, SimpleNamespace, list[float]]:\n",
    "    # Initialize random seed\n",
    "    np.random.seed(config.seed)\n",
    "    random.seed(config.seed)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = gym.make(config.env_name)\n",
    "    env = EnvironmentNormalizer(env)\n",
    "    \n",
    "    try: \n",
    "        # Initialize control algorithm\n",
    "        control_algorithm = build_control_experiment(\n",
    "            env=env,\n",
    "            method=config.control_algorithm.method,\n",
    "            gamma=config.gamma,\n",
    "            function=config.control_algorithm.function,\n",
    "            num_episodes=config.num_episodes,\n",
    "            replay_capacity=config.replay_capacity,\n",
    "            batch_size=config.batch_size,\n",
    "            n0=config.n0,\n",
    "            discrete_scale=config.discrete_scale,\n",
    "            eps_func=config.eps_func,\n",
    "            stochasticity_factor=config.stochasticity_factor,\n",
    "            method_args=config.control_algorithm.args.__dict__,\n",
    "        )\n",
    "    except Exception:\n",
    "        return ExperimentExitCode.INVALID, config, []\n",
    "    \n",
    "    # Run control algorithm\n",
    "    try:\n",
    "        eps_rewards = control_algorithm.fit()\n",
    "    except Exception:\n",
    "        return ExperimentExitCode.FAILED, config, []\n",
    "    \n",
    "    return ExperimentExitCode.SUCCESS, config, eps_rewards"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T14:05:44.361947775Z",
     "start_time": "2023-12-19T14:05:44.290894959Z"
    }
   },
   "id": "3e86f45279c3984a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def experiment_generator_grid_search() -> SimpleNamespace:\n",
    "    for env_name in [\"CartPole-v0\"]:\n",
    "        for method in [\"q-learning\", \"monte-carlo\", \"sarsa-lambda\"]:\n",
    "            for function in [\"linear\", \"tabular\", None]:\n",
    "                for gamma in [0.5, 0.95]:\n",
    "                    for λ in [None, 0.3, 0.6, 0.9]:\n",
    "                        for n0 in [2, 5, 10, 25, None]:\n",
    "                            for eps_func in [\"s\"]:\n",
    "                                for discrete_scale in [2, 5, 20]:\n",
    "                                #for stochasticity_factor in [0.0, 0.25, 0.5]:\n",
    "                                #    for reward_mode in [\"normal\", \"sparse\"]:\n",
    "                                        for seed in range(3): \n",
    "                                            yield SimpleNamespace(\n",
    "                                                env_name=env_name,\n",
    "                                                num_episodes=10 if method != \"dqn\" else 2_000,\n",
    "                                                control_algorithm=SimpleNamespace(\n",
    "                                                    method=method, \n",
    "                                                    args=SimpleNamespace(\n",
    "                                                        lambda_factor=λ,\n",
    "                                                    ),\n",
    "                                                    function=function,\n",
    "                                                ),\n",
    "                                                gamma=gamma,\n",
    "                                                replay_capacity=10_000,\n",
    "                                                batch_size=1 if method != \"dqn\" else 128,\n",
    "                                                discrete_scale=discrete_scale,\n",
    "                                                n0=n0,\n",
    "                                                eps_func=eps_func,\n",
    "                                                stochasticity_factor=0.0,\n",
    "                                                reward_mode=\"normal\",\n",
    "                                                seed=seed\n",
    "                                            )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T14:05:44.437920296Z",
     "start_time": "2023-12-19T14:05:44.361168720Z"
    }
   },
   "id": "7ffab95b19eb607b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3240 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17c30dc67ce04210ad52c2bd01ed84e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "TypeError: issubclass() arg 1 must be a class\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/data_lids/home/ilansilva/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/data_lids/home/ilansilva/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/numpy/core/numerictypes.py\", line 319, in issubclass_\n",
      "    return issubclass(arg1, arg2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/mambaforge/lib/python3.10/multiprocessing/pool.py:856\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 856\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_items\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpopleft\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Pool(n_cpu) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m exit_code, config, eps_rewards \u001B[38;5;129;01min\u001B[39;00m tqdm(p\u001B[38;5;241m.\u001B[39mimap_unordered(run, experiments), total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(experiments)):\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m exit_code \u001B[38;5;241m==\u001B[39m ExperimentExitCode\u001B[38;5;241m.\u001B[39mSUCCESS:\n\u001B[1;32m      9\u001B[0m             results\u001B[38;5;241m.\u001B[39mappend((config, eps_rewards))\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/tqdm/notebook.py:249\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    248\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m    252\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/rl-final-project-VQ_uW-z5-py3.10/lib/python3.10/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/lib/python3.10/multiprocessing/pool.py:861\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 861\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    863\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_items\u001B[38;5;241m.\u001B[39mpopleft()\n",
      "File \u001B[0;32m~/mambaforge/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "experiments = list(experiment_generator_grid_search())\n",
    "results = []\n",
    "\n",
    "with Pool(n_cpu) as p:\n",
    "    for exit_code, config, eps_rewards in tqdm(p.imap_unordered(run, experiments), total=len(experiments)):\n",
    "        if exit_code == ExperimentExitCode.SUCCESS:\n",
    "            results.append((config, eps_rewards))\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T14:06:00.354653572Z",
     "start_time": "2023-12-19T14:05:44.436170929Z"
    }
   },
   "id": "1f757e3a8f176235"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T14:06:00.352630211Z"
    }
   },
   "id": "fae745781865802e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
